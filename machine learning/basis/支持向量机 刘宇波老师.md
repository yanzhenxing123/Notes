## 决策边界

泛化能力可能不行

![image-20200726164626772](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/112520-961617.png)

![image-20200726164634715](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/112528-496357.png)

## 支持向量机背后的最优化问题

svm最大化 ![[公式]](https://www.zhihu.com/equation?tex=margin) ， ![[公式]](https://www.zhihu.com/equation?tex=margin%3D2d) ，就对应于最大化d，也就是点到直线的距离最大。回忆解析几何，在二维空间中点 ![[公式]](https://www.zhihu.com/equation?tex=%28x%EF%BC%8Cy%29) 到直线 ![[公式]](https://www.zhihu.com/equation?tex=Ax%2BBy%2BC%3D0) 的距离公式 :![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cmid+Ax%2BBy%2BC+%5Cmid%7D+%7B%5Csqrt%7BA%5E2%2BB%5E2%7D%7D)

将其拓展到n维： ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta%5ET+%5Ccdot+x_%7Bb%7D%3D0)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 包含截距和系数， ![[公式]](https://www.zhihu.com/equation?tex=x_%7Bb%7D) 就是在 ![[公式]](https://www.zhihu.com/equation?tex=x) 样本中加入一行常数1，这就跟之前的线性回归中是一样的。

如果将截距提出来就是 ![[公式]](https://www.zhihu.com/equation?tex=w%5ETx+%2B+b+%3D+0)

其中的 ![[公式]](https://www.zhihu.com/equation?tex=w) 就是对样本中的每一个数据赋予了一个权值。这就是直线方程的两种不同表示方式。那么，由此可以得到新的点到直线距离方程：

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cmid+w%5ETx+%2B+b+%5Cmid%7D%7B%5Cparallel+w+%5Cparallel%7D) ，其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Cparallel+w+%5Cparallel+%3D+%5Csqrt+%7Bw_%7B1%7D%5E2+%2B+w_%7B2%7D%5E2+%2B+%5Cdots+%2B+w_%7Bn%7D%5E2%7D)









![image-20200726165517011](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/112602-865857.png)

除分母

![image-20200726165630097](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/112610-422956.png)

![image-20200726165832385](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/112812-571569.png)

![image-20200726165911655](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/112815-427164.png)

![[公式]](https://www.zhihu.com/equation?tex=%5Cboxed+%7By_%7Bi%7D+%28w%5ETx%5E%7B%28i%29%7D+%2B+b%29+%5Cgeq+1%7D)

因此对于任意支持向量 ![[公式]](https://www.zhihu.com/equation?tex=x) 都要 ![[公式]](https://www.zhihu.com/equation?tex=max+%5Cfrac%7B%5Cmid+w%5ETx+%2Bb+%5Cmid%7D%7B%5Cparallel+w+%5Cparallel%7D) ，无论 ![[公式]](https://www.zhihu.com/equation?tex=x) 属于样本1还是样本-1，最小值都是1，所以转化为最大化 ![[公式]](https://www.zhihu.com/equation?tex=max+%5Cfrac%7B1%7D%7B%5Cparallel+w+%5Cparallel%7D) ,也即是最小化 ![[公式]](https://www.zhihu.com/equation?tex=min%5Cparallel+w+%5Cparallel) 。

但是往往为了求导方便，通常都是最小化 ![[公式]](https://www.zhihu.com/equation?tex=%5Cboxed%7Bmin%5Cfrac%7B1%7D%7B2%7D+%5Cparallel+w+%5Cparallel%5E2%7D) ，但是这是一个有约束条件的最优化问题，就是要满足 ![[公式]](https://www.zhihu.com/equation?tex=s.t.+%5Cquad+y_%7Bi%7D+%28w%5ETx%5E%7B%28i%29%7D+%2B+b%29+%5Cgeq+1%2C+%5Cquad+i%3D1%2C2%2C%5Cdots%2Cm)

要找到具有最大间隔的划分超平面，即是找到能满足条件约束的参数 ![[公式]](https://www.zhihu.com/equation?tex=w) 和 ![[公式]](https://www.zhihu.com/equation?tex=b) ，使得 ![[公式]](https://www.zhihu.com/equation?tex=min%5Cfrac%7B1%7D%7B2%7D+%5Cparallel+w+%5Cparallel%5E2) ，这就是支持向量机的基本型。



## SVM中使用多项式特征

pass



## 什么是核函数

**SVM本质就是求最优解**

核函数的目的：将线性不可分数据变为线性可分的数据

eg：

![image-20200727142848961](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/112820-409735.png)







**第一步：向高维空间映射**

令 ![[公式]](https://www.zhihu.com/equation?tex=%5Cphi%28x%29) 表示将映射后的特征向量，于是在特征空间中划分超平面所对应的的模型可表示为

![[公式]](https://www.zhihu.com/equation?tex=f%28x%29+%3D+w%5ET+%5Cphi%28x%29+%2B+b) ,

其中 ![[公式]](https://www.zhihu.com/equation?tex=w) 和 ![[公式]](https://www.zhihu.com/equation?tex=b) 对应模型参数，有约束条件：

![[公式]](https://www.zhihu.com/equation?tex=min%5Cfrac%7B1%7D%7B2%7D+%5Cparallel+w+%5Cparallel%5E2%EF%BC%8C+%5Cquad+s.t.+%5Cquad+y_%7Bi%7D+%28w%5ET+%5Cphi%28x%5E%7B%28i%29%7D%29+%2B+b%29+%5Cgeq+1)

其对偶问题：

![[公式]](https://www.zhihu.com/equation?tex=max%5Csum_%7Bi%3D1%7D%5Em%5Calpha_%7Bi%7D+-+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5Em+%5Csum_%7Bj%3D1%7D%5Em+%5Calpha_%7Bi%7D+%5Calpha_%7Bj%7D+y_%7Bi%7D+y_%7Bj%7DK%28x_%7Bi%7D%2Cx_%7Bj%7D%29%2C+%5Cquad+s.t.+0+%5Cleq+%5Calpha_%7Bi%7D+%5Cleq+C+%5Cquad+%5Csum_%7Bi%3D1%7D%5Em+%5Calpha_%7Bi%7Dy_%7Bi%7D%3D0)

**第二步：对偶问题的证明过程**

![[公式]](https://www.zhihu.com/equation?tex=L%28w%2C+b%2C+%5Calpha%29+%3D+%5Cfrac%7B1%7D%7B2%7D+%5Cparallel+w+%5Cparallel%5E2+%2B+%5Csum_%7Bi%3D1%7D%5Em+%5Calpha_%7Bi%7D+%281-y_%7Bi%7D%28w%5ETx_%7Bi%7D%2Bb%29%29)

令 ![[公式]](https://www.zhihu.com/equation?tex=L%28w%2Cb%2C%5Calpha%29) 对 ![[公式]](https://www.zhihu.com/equation?tex=w) 和 ![[公式]](https://www.zhihu.com/equation?tex=b) 的偏导为零可得，

求解过程会用到的数学知识：

![[公式]](https://www.zhihu.com/equation?tex=Y%3DA+%2A+X+%5Cquad+-%3E%3E+%5Cquad+%5Cfrac%7BdY%7D%7BdX%7D+%3D+A%5ET++%5Ctag%7B1%7D)

![[公式]](https://www.zhihu.com/equation?tex=Y%3DX%2AB+%5Cquad+-%3E%3E+%5Cquad+%5Cfrac%7BdY%7D%7BdX%7D+%3D+B%5ET+%5Ctag%7B2%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L%28w%2C+b%2C+%5Calpha%29%7D%7B%5Cpartial+w%7D+%3D+%5Cfrac%7B%5Cpartial+%5Csum_%7Bi%3D1%7D%5Em%28%5Calpha_%7Bi%7D+-+%5Calpha_%7Bi%7D+y_%7Bi%7D%28w%5ETx_%7Bi%7D%2Bb%29%29%7D%7B%5Cpartial+w%7D%3D+%5Calpha_%7Bi%7D+y_%7Bi%7D%5Cfrac%7B%5Cpartial+%5Csum_%7Bi%3D1%7D%5Em%28w%5ETx_%7Bi%7D%29%7D%7B%5Cpartial+w%7D%3D%28w%5ET%29%5ET+%3D+w)

![[公式]](https://www.zhihu.com/equation?tex=w+%3D+%5Csum_%7Bi%3D1%7D%5Em+%5Calpha_%7Bi%7D+y_%7Bi%7D+x_%7Bi%7D+%5Ctag%7B3%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L%28w%2C+b%2C+%5Calpha%29%7D%7B%5Cpartial+b%7D%3D%5Cfrac%7B%5Cpartial+%5Csum_%7Bi%3D1%7D%5Em%28%5Calpha_%7Bi%7D+-+%5Calpha_%7Bi%7D+y_%7Bi%7D%28w%5ETx_%7Bi%7D%2Bb%29%29%7D%7B%5Cpartial+b%7D%3D+%5Cfrac%7B%5Cpartial+%5Csum_%7Bi%3D1%7D%5Em%28%5Calpha_%7Bi%7D+y_%7Bi%7Db%29%7D%7B%5Cpartial+b%7D%3D+%5Csum_%7Bi%3D1%7D%5Em+%5Calpha_%7Bi%7D+y_%7Bi%7D+%3D+0)

![[公式]](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5Em+%5Calpha_%7Bi%7D+y_%7Bi%7D+%3D+0+%5Ctag%7B4%7D)

将（3）（4）代入到 ![[公式]](https://www.zhihu.com/equation?tex=L%28w%2Cb%2C%5Calpha%29) 将 ![[公式]](https://www.zhihu.com/equation?tex=w) 和 ![[公式]](https://www.zhihu.com/equation?tex=b) 消去，得到**对偶问题**：

![[公式]](https://www.zhihu.com/equation?tex=max%5Csum_%7Bi%3D1%7D%5Em%5Calpha_%7Bi%7D+-+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5Em+%5Csum_%7Bj%3D1%7D%5Em+%5Calpha_%7Bi%7D+%5Calpha_%7Bj%7D+y_%7Bi%7D+y_%7Bj%7D+x_%7Bi%7D+x_%7Bj%7D%2C+%5Cquad+s.t.+0+%5Cleq+%5Calpha_%7Bi%7D+%5Cleq+C+%5Cquad+%5Csum_%7Bi%3D1%7D%5Em+%5Calpha_%7Bi%7Dy_%7Bi%7D%3D0+)

![[公式]](https://www.zhihu.com/equation?tex=max%5Csum_%7Bi%3D1%7D%5Em%5Calpha_%7Bi%7D+-+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5Em+%5Csum_%7Bj%3D1%7D%5Em+%5Calpha_%7Bi%7D+%5Calpha_%7Bj%7D+y_%7Bi%7D+y_%7Bj%7DK%28x_%7Bi%7D%2Cx_%7Bj%7D%29%2C+%5Cquad+s.t.+0+%5Cleq+%5Calpha_%7Bi%7D+%5Cleq+C+%5Cquad+%5Csum_%7Bi%3D1%7D%5Em+%5Calpha_%7Bi%7Dy_%7Bi%7D%3D0+)

**第三步：**经过对偶问题的求解后即可得到，

![[公式]](https://www.zhihu.com/equation?tex=f%28x%29+%3D+w%5ET+%5Cphi%28x%29+%2B+b+%3D+%5Csum_%7Bi%3D1%7D%5Em+%5Calpha_%7Bi%7D+y_%7Bi%7D+%5Cphi%28x_i%29%5ET+%5Cphi%28x%29+%2B+b+%3D+%5Csum_%7Bi%3D1%7D%5Em+%5Calpha_%7Bi%7D+y_%7Bi%7D+k%28x%2C+x_%7Bi%7D%29+%2B+b+)

------

常用 核函数：

1、线性核： ![[公式]](https://www.zhihu.com/equation?tex=k%28x_%7Bi%7D%2C+x_%7Bj%7D%29+%3D+x_%7Bi%7D%5ET+x_%7Bj%7D+%2B+c)

2、多项式核： ![[公式]](https://www.zhihu.com/equation?tex=k%28x_%7Bi%7D%2C+x_%7Bj%7D%29+%3D+%28x_%7Bi%7D%5ET+x_%7Bj%7D+%2B+c%29%5Ed) ，当d=1时退化为线性核。

3、高斯核RBF： ![[公式]](https://www.zhihu.com/equation?tex=k%28x_%7Bi%7D%2C+x_%7Bj%7D%29+%3D+exp%28-%5Cfrac%7B%5Cparallel+x_%7Bi%7D+-+x_%7Bj%7D+%5Cparallel+%5E2%7D%7B2%5Csigma+%5E2%7D%29%2C+%5Csigma+%3E0) 为高斯核的带宽

RBF核：Radial Basis Function Kernel

4、拉普拉斯核： ![[公式]](https://www.zhihu.com/equation?tex=k%28x_%7Bi%7D%2C+x_%7Bj%7D%29+%3D+exp%28-%5Cfrac%7B%5Cparallel+x_%7Bi%7D+-+x_%7Bj%7D+%5Cparallel%7D%7B%5Csigma%7D%29%EF%BC%8C+%5Csigma+%3E+0)

5、Sigmoid核： ![[公式]](https://www.zhihu.com/equation?tex=k%28x_%7Bi%7D%2C+x_%7Bj%7D%29+%3D+tanh%28%5Cbeta+x_%7Bi%7D%5ET+x_%7Bj%7D+%2B+%5Ctheta%29)

接下里重点介绍高斯核函数。









![image-20200727141558447](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/112828-824712.png)

![image-20200727141639981](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/112831-448575.png)



![image-20200727142730632](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/113006-435357.png)

### 高斯核函数

![image-20200727143219007](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/113018-748169.png)

其实，真正的高斯核函数实现的过程中并不是固定的landmark，而是对于每一个数据点都是landmark。

![image-20200727144304442](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/113104-987829.png)



gamma相当于是在调节模型的复杂度，gamma越小模型复杂度越低，gamma越高模型复杂度越高。因此需要调节超参数gamma平衡过拟合和欠拟合。





## 使用SVM解决回归问题

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()
x = boston.data
y = boston.target

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=666)

from sklearn.svm import SVR
from sklearn.svm import LinearSVR
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

def StandardLinearSVR(epsilon=0.1):
    return Pipeline([
        ('std_scale', StandardScaler()),
        # C, kernel, 等超参需要调节
        ('linear_svr', LinearSVR(epsilon=epsilon))
    ])

svr = StandardLinearSVR()
svr.fit(x_train, y_train)
# 0.6357154352424855
```