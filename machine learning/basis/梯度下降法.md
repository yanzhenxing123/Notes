# 机器学习算法———梯度下降法

## 局部最优解和全局最优解

![image-20200721154737755](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/112429-448588.png)



## 线性回归中使用梯度下降法

损失函数：

![image-20200721194830357](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/112435-385783.png)

y帽

![image-20200721194856856](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/112441-75325.png)

![image-20200721194939119](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/114656-490764.png)





![image-20200721164009284](C:\Users\20924\AppData\Roaming\Typora\typora-user-images\image-20200721164009284.png)

```python
import numpy as np
import matplotlib.pyplot as plt

# 每一次生成的随机数相同 
np.random.seed(666)
x = 2 * np.random.random(size=100)
y = x * 3. + 4. + np.random.normal(size=100)

# (100, 1)
X = x.reshape(-1, 1)
X.shape
y.shape
plt.scatter(x, y)
plt.show()

# 损失函数
def J(theta, X_b, y):
    try:
        return np.sum((y - X_b.dot(theta)) ** 2) / len(X_b)
    except:
        return float('inf')

# 求导数
def dj(theta, X_b, y):
    res = np.empty(len(theta))
    res[0] = np.sum(X_b.dot(theta) - y)
    for i in range(1, len(theta)):
        res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])

    return res * 2 / len(X_b)

# 梯度下降过程
def gradient_descent(X_b, y, init_theta, eta, n_iters=1e4, epsilon=1e-8):

    theta = init_theta
    i_iter = 0

    while i_iter < n_iters:
        gradient = dj(theta, X_b, y)
        last_theta = theta
        theta = last_theta - eta * gradient

        if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
            break

        i_iter += 1

    return theta

# X_b就是将x和1横着加上
X_b = np.hstack([np.ones((len(X), 1)), X])
init_theta = np.zeros(X_b.shape[1])
eta = 0.01
theta = gradient_descent(X_b, y, init_theta, eta)
theta

```

`plt.show()`

![image-20200721200126190](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/114701-619083.png)



## 随机梯度下降法

**模拟退火的思想**



![image-20200721201621084](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/114703-622369.png)

```python
# 传的是某一行
def dj_sgd(theta, X_b_i, y_i):
    return X_b_i.T.dot(X_b_i.dot(theta) - y_i) * 2.

# 只对次数做出判定
def sgd(X_b, y, init_theta, n_iters):
    t0 = 5
    t1 = 50

    def learning_rate(t):
        return t0 / (t + t1)

    theta = init_theta
    for cur_iter in range(n_iters):
        rand_i = np.random.randint(len(X_b))
        gradient = dj_sgd(theta, X_b[rand_i], y[rand_i])
        theta = theta - learning_rate(cur_iter) * gradient

    return theta
```