# 多项式回归

## basis

**思想**：主要的思想还是把其看作线性回归，将x^2视为另一个特征值





## 代码实现

### 利用线性回归实现

```python
from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(X, y)
y_predict = lin_reg.predict(X)
plt.scatter(x, y)
plt.plot(x, y_predict, color='r')
plt.show()
lin_reg2.coef_
# array([0.90802935, 1.04112467])
lin_reg2.intercept_
# 2.3783560083768602
```

### sklearn中实现

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.random.uniform(-3, 3, size=100)
X = x.reshape(-1, 1)
y = 0.5 + x**2 + x + 2 + np.random.normal(0, 1, size=100)
# 多项式回归其实对数据进行预处理，给数据添加新的特征，所以调用的库在preprocessing中
from sklearn.preprocessing import PolynomialFeatures

# 这个degree表示我们使用多少次幂的多项式
poly = PolynomialFeatures(degree=2)    
poly.fit(X)
X2 = poly.transform(X)
X2.shape

from sklearn.linear_model import LinearRegression
reg = LinearRegression() 
reg.fit(X2, y)
y_predict = reg.predict(X2) 
plt.scatter(x, y) 
plt.plot(np.sort(x), y_predict2[np.argsort(x)], color='r')
plt.show()
```

### 可以直接使用pipline



![image-20200722164732821](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/20/113948-680906.png)



## 过拟合和欠拟合

overfitting and underfitting

拟合过强和拟合过弱



### 泛化能力

利用测试数据进行评定



### 模型复杂度和模型准确率





![image-20200722185936366](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/20/114001-413642.png)



## **学习曲线**

![image-20200722190229455](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/20/114006-242656.png)

![image-20200722192035202](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/20/114028-973690.png)

![image-20200722192045450](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/20/114010-198294.png)



## 验证数据集和交叉验证

### 验证数据集

![image-20200722192641689](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/20/114050-421843.png)

### 交叉验证

![image-20200722192724827](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111626-99456.png)

![image-20200722194244260](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111629-648601.png)

## 偏差和方差

非参数学习：高方差，过拟合

参数学习：高偏差， 欠拟合

![image-20200722194627752](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111631-410624.png)

![image-20200722194853710](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111722-709998.png)

## 模型正则化: 限制参数的大小

- 加入正则项之前

目标：使 ![[公式]](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5Em%28y%5E%7B%28i%29%7D+-+%5Ctheta_%7B0%7D+-+%5Ctheta_%7B1%7DX_%7B1%7D%5E%7B%28i%29%7D+-+%5Ctheta_%7B2%7DX_%7B2%7D%5E%7B%28i%29%7D++-+%5Cdots+-+%5Ctheta_%7Bn%7DX_%7Bn%7D%5E%7B%28i%29%7D%29%5E2) 尽可能的小，

目标：使 ![[公式]](https://www.zhihu.com/equation?tex=J%28%5Ctheta%29+%3D+MSE%28y%2C+%5Chat+y%EF%BC%9B+%5Ctheta%29) 尽可能地小

- 加入正则项之后

目标： ![[公式]](https://www.zhihu.com/equation?tex=J%28%5Ctheta%29+%3D+MSE%28y%2C+%5Chat+y%EF%BC%9B+%5Ctheta%29+%2B+%5Calpha+%5Cfrac%7B1%7D%7B2%7D+%5Csum_%7Bi%3D1%7D%5En+%5Ctheta_%7Bi%7D%5E2) 尽可能小

通过加入的正则项来控制系数不要太大，从而使曲线不要那么陡峭，变化的那么剧烈。在这里有几个细节需要注意。

- **第一点：**![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)**从1开始，只包含系数不包括截距，这是因为截距只决定曲线的高低，并不会影响曲线的陡峭和缓和。**
- **第2点：就是这个**![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D)**，是为了求导之后能够将2消去，为了方便计算。不过其实这个有没有都是可以的，因为在正则化前有一个系数，我们可以把这个**![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D)**可以考虑到**![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)**中去。**
- **第3点：系数**![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)**，它表示正则化项在整个损失函数中所占的比例。极端一下，**![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)**=0时，相当于模型没有加入正则化，但如果**![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)**= 正无穷，此时其实主要的优化任务就变成了需要所有的**![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)**都尽可能的小，最优的情况就是全为0。至于**![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)**的取值就需要尝试了。**

### 岭回归

![image-20200725170704800](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111727-690330.png)

*使用岭回归*

```python
from sklearn.linear_model import Ridge

def RidgeRegression(degree, alpha):
    return Pipeline([
        ('poly', PolynomialFeatures(degree=degree)),
        ('std_scale', StandardScaler()),
        ('lin_reg', Ridge(alpha=alpha)),
    ])

ridege1_reg = RidgeRegression(20, alpha=0.0001)
ridege1_reg.fit(x_train, y_train)

y1_predict = ridege1_reg.predict(x_test)
mean_squared_error(y_test, y1_predict)
# 1.3233492754136291
# 跟之前的136.相比小了很多
plot_model(ridege1_reg)
```

### LASSO回归

**比较：**

![image-20200725183709905](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111730-458957.png)

使用岭回归趋向于曲线，Lasso趋向于直线，即将一些项的系数设置为0

![image-20200725184046411](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111732-401375.png)

岭回归更为准确

- ![[公式]](https://www.zhihu.com/equation?tex=Rigde%EF%BC%9A%5Cfrac%7B1%7D%7B2%7D%5Csum_%7Bi%3D1%7D%5En+%5Ctheta_%7Bi%7D%5E2+%5Cqquad++%5Cqquad+Lasso%3A+%5Csum_%7Bi%3D1%7D%5En+%5Cmid+%5Ctheta_%7Bi%7D+%5Cmid+)
- ![[公式]](https://www.zhihu.com/equation?tex=MSE%3A+%5Cfrac%7B1%7D%7Bn%7D%5Csum_%7Bi%3D1%7D%5En%28y_%7Bi%7D+-+%5Chat+y_%7Bi%7D%29%5E2++%5Cqquad+MAE%3A+%5Cfrac%7B1%7D%7Bn%7D+%5Csum_%7Bi%3D1%7D%5En+%5Cmid+%28y_%7Bi%7D+-+%5Chat+y_%7Bi%7D%29+%5Cmid)
- ![[公式]](https://www.zhihu.com/equation?tex=+%E6%AC%A7%E6%8B%89%E8%B7%9D%E7%A6%BB%EF%BC%9A%5Csqrt%7B+%5Csum_%7Bi%3D1%7D%5En++%28x_%7Bi%7D%5E%7B%281%29%7D+-+x_%7Bi%7D%5E%7B%282%29%7D%29+%5E2+%7D+%5Cqquad+%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB%EF%BC%9A%5Csum_%7Bi%3D1%7D%5En+%5Cmid++%28x_%7Bi%7D%5E%7B%281%29%7D+-+x_%7Bi%7D%5E%7B%282%29%7D%29+%5Cmid++)



## 弹性网络

![image-20200725184918161](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111740-520012.png)

**均方误差**： MSE

**平均绝对误差**：MAE





![image-20200725185652522](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111746-9237.png)

