## Docker网络

#### 理解网络Docker0

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200615111107627.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpeWFuXzA4MTY=,size_16,color_FFFFFF,t_70#pic_center)

三个网络

```shell
# docker 是如何处理容器网络访问的？
1
[root@server /]# docker run -d -P --name tomcat01 tomcat
Digest: sha256:ce753be7b61d86f877fe5065eb20c23491f783f283f25f6914ba769fee57886b
Status: Downloaded newer image for tomcat:latest
7b1d0acb9d1b573b2380d56fed4517b98b7084060f7ae4d022f449802884e03d
#查看容器的内部网址 ip addr 
12345
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200615111137927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpeWFuXzA4MTY=,size_16,color_FFFFFF,t_70#pic_center)

```shell
#linux能不能ping通容器内部？
[root@server /]# docker exec tomcat01 ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
120: eth0@if121: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default 
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever
[root@server /]# ping 172.17.0.2
PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.
64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.066 ms
64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.048 ms
64 bytes from 172.17.0.2: icmp_seq=3 ttl=64 time=0.048 ms
64 bytes from 172.17.0.2: icmp_seq=4 ttl=64 time=0.047 ms
#可以ping通容器内部
1234567891011121314151617
```

> 原理

1. 我们每启动一个docker容器，docker就会给我们的容器分配一个ip地址，我们只要安装了docker，就会有一个网卡docker0

   桥接模式，使用的技术是evth-pair技术

2. 在每开启运行一个容器，就会增加一对儿对应的网卡

   ```shell
   evth-pair 就是一对的虚拟设备接口，他们都是成对出现的，一段连着协议，一段彼此相连，正因为有这个特性，evth-pair充当一个桥梁，专门连接各种虚拟网络设备
   1
   ```

3. 容器和容器之间是可以互相ping通的

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200615111213373.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpeWFuXzA4MTY=,size_16,color_FFFFFF,t_70#pic_center)
​ tomcat01和tomcat02是公用的一个路由器即docker0

 所有的容器在不指定网络的情况下，都是docker0路由的，docker0会给我们的 容器分配一个默认的可用ip

> 小结

Docker使用的是Linux的桥接，宿主机zhong是一个Docker容器的网桥docker0

docker中的所有网络接口都是虚拟的，虚拟的转发效率高

只要容器删除，对应网桥也会自动删除

#### –link

> 思考一个场景，我们编写了一个微服务，database url=ip 项目不重启，数据库ip掉了，我们希望可以处理这个问题，可不可以用名字来访问？

```shell
[root@server /]# docker exec tomcat01 ping tomcat02
ping: tomcat02: Name or service not known

==============#如何解决容器间通讯？  --link====================

[root@server /]# docker run -d -P --name tomcat03 --link tomcat02 tomcat
dbe1b8ea01c8e07d3cb3539b97452ecfaca3ec8aba7666ea39172f8b409afab7
[root@server /]# docker exec  tomcat03 ping tomcat02
PING tomcat02 (172.17.0.3) 56(84) bytes of data.
64 bytes from tomcat02 (172.17.0.3): icmp_seq=1 ttl=64 time=0.078 ms
64 bytes from tomcat02 (172.17.0.3): icmp_seq=2 ttl=64 time=0.052 ms
64 bytes from tomcat02 (172.17.0.3): icmp_seq=3 ttl=64 time=0.053 ms
=============#会有一个问题，反向会有ping通么？ 不通===================
[root@server /]# docker exec tomcat02 ping tomcat03
ping: tomcat03: Name or service not known
123456789101112131415
#其实是这个tomcat03在本地配置了tomcat02，但是tomcat02没有配置tomcat03
[root@server /]# docker exec tomcat03 cat /etc/hosts
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters
172.17.0.3	tomcat02 819c0cc99696
172.17.0.4	dbe1b8ea01c8
[root@server /]# docker exec tomcat02 cat /etc/hosts
127.0.0.1	localhost
::1	localhost ip6-localhost ip6-loopback
fe00::0	ip6-localnet
ff00::0	ip6-mcastprefix
ff02::1	ip6-allnodes
ff02::2	ip6-allrouters
172.17.0.3	819c0cc99696
123456789101112131415161718
```

–link：就是在host配置中增加了一个tomcat02的映射

我们现在玩Docker已经不建议使用–link了

我们玩的是自定义网络，因为docker0不支持容器名连接访问

#### 自定义网络

> 查看所有的docker网络

```shell
[root@server ~]# docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
d2a0a2fb88a5        bridge              bridge              local
bf5d1c08782b        host                host                local
bbfe8c38fb6a        none                null                local
12345
```

**网络模式**

birdge：桥接 （默认，自己创建也使用这个）

none：不配置网络

host：主机模式（和宿主机共享网络）

container：容器网络连通（用的很少，局限性很大）

```shell
[root@server ~]# docker network --help

Usage:	docker network COMMAND

Manage networks

Commands:
  connect     将容器连接到网络
  create      创建一个网络
  disconnect  断开容器与网络的链接
  inspect     在一个或多个网络上显示详细信息
  ls          列出全部网络
  prune       删除所有未使用网络
  rm          染出一个或多个网络
1234567891011121314
```

**测试**

```shell
#我们直接启动的命令，默认也是添加了（--net birdge，也就是docker0）
[root@server ~]# docker run -d -P --name tomcat01 tomcat
[root@server ~]# docker run -d -P --name tomcat01 --net birdge tomcat
#docker0：默认的，域名不能访问，虽然--link可以打通连接，但是比较麻烦，不推荐
===============自定义网络============
#--driver birdge 桥接模式
#--subnet	子网掩码
#--gateway 网关
[root@server ~]# docker network create --driver birdge --subnet 192.168.0.0/16 --gateway 192.168.0.1 mynet
1c43dc4178dde11c0c24580adaa880f90a1d740dbe6d77b655f7879a196746fa
#创建成功
[root@server ~]# docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
d2a0a2fb88a5        bridge              bridge              local
bf5d1c08782b        host                host                local
1c43dc4178dd        mynet               bridge              local
bbfe8c38fb6a        none                null                local
#自己的网络已经创建好了
[root@server ~]# docker network  inspect mynet 
[
    {
        "Name": "mynet",
        "Id": "1c43dc4178dde11c0c24580adaa880f90a1d740dbe6d77b655f7879a196746fa",
        "Created": "2020-05-27T16:28:39.844562581+08:00",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": {},
            "Config": [
                {
                    "Subnet": "192.168.0.0/16",
                    "Gateway": "192.168.0.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Containers": {},
        "Options": {},
        "Labels": {}
    }
]
#基于自定义的网络启动两个tomcat容器
[root@server ~]# docker run -d -P --name tomcat-net01 --net mynet tomcat
0b67665aab67bd4f46c64bce14fe1170229c12c60886abd4c2cd650de8438a7c
[root@server ~]# docker run -d -P --name tomcat-net02 --net mynet tomcat
292028f5c40ae10dcc011bef924bee872c4cb84252e65ccf496d0f7fe760096b
#再次查看我们自定义的网络详情
[root@server ~]# docker network inspect mynet 
[
    {
        "Name": "mynet",
        "Id": "1c43dc4178dde11c0c24580adaa880f90a1d740dbe6d77b655f7879a196746fa",
        "Created": "2020-05-27T16:28:39.844562581+08:00",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": {},
            "Config": [
                {
                    "Subnet": "192.168.0.0/16",
                    "Gateway": "192.168.0.1"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Containers": {
            "0b67665aab67bd4f46c64bce14fe1170229c12c60886abd4c2cd650de8438a7c": {
                "Name": "tomcat-net01",
                "EndpointID": "7b07b8abf25462a5dc683795df627a3b7f3b3c8ffce20915ad2a98fa6a83f83d",
                "MacAddress": "02:42:c0:a8:00:02",
                "IPv4Address": "192.168.0.2/16",
                "IPv6Address": ""
            },
            "292028f5c40ae10dcc011bef924bee872c4cb84252e65ccf496d0f7fe760096b": {
                "Name": "tomcat-net02",
                "EndpointID": "b1e61271f8651d32dc847c6ca3df55b191813c79a73b028db510dcc6184cefe4",
                "MacAddress": "02:42:c0:a8:00:03",
                "IPv4Address": "192.168.0.3/16",
                "IPv6Address": ""
            }
        },
        "Options": {},
        "Labels": {}
    }
]
======================两个容器网络连接测试======================
#tomcat-net01 ping tomcat-net02的ip地址：可以ping通
[root@server ~]# docker exec tomcat-net01 ping 192.168.0.3
PING 192.168.0.3 (192.168.0.3) 56(84) bytes of data.
64 bytes from 192.168.0.3: icmp_seq=1 ttl=64 time=0.075 ms
64 bytes from 192.168.0.3: icmp_seq=2 ttl=64 time=0.053 ms
64 bytes from 192.168.0.3: icmp_seq=3 ttl=64 time=0.035 ms
64 bytes from 192.168.0.3: icmp_seq=4 ttl=64 time=0.046 ms
#tomcat-net01 ping tomcat-net02的名字：可以ping通
[root@server ~]# docker exec tomcat-net01 ping tomcat-net02
PING tomcat-net02 (192.168.0.3) 56(84) bytes of data.
64 bytes from tomcat-net02.mynet (192.168.0.3): icmp_seq=1 ttl=64 time=0.034 ms
64 bytes from tomcat-net02.mynet (192.168.0.3): icmp_seq=2 ttl=64 time=0.059 ms
64 bytes from tomcat-net02.mynet (192.168.0.3): icmp_seq=3 ttl=64 time=0.050 ms
64 bytes from tomcat-net02.mynet (192.168.0.3): icmp_seq=4 ttl=64 time=0.052 ms
#tomcat-net02 ping tomcat-net01的名字：可以ping通
[root@server ~]# docker exec tomcat-net02 ping tomcat-net01
PING tomcat-net01 (192.168.0.2) 56(84) bytes of data.
64 bytes from tomcat-net01.mynet (192.168.0.2): icmp_seq=1 ttl=64 time=0.035 ms
64 bytes from tomcat-net01.mynet (192.168.0.2): icmp_seq=2 ttl=64 time=0.050 ms
64 bytes from tomcat-net01.mynet (192.168.0.2): icmp_seq=3 ttl=64 time=0.051 ms
#tomcat-net02 ping tomcat-net01的名ip地址：可以ping通
[root@server ~]# docker exec tomcat-net02 ping 192.168.0.2
PING 192.168.0.2 (192.168.0.2) 56(84) bytes of data.
64 bytes from 192.168.0.2: icmp_seq=1 ttl=64 time=0.052 ms
64 bytes from 192.168.0.2: icmp_seq=2 ttl=64 time=0.054 ms
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127
```

**自定义网络的好处**

不同的集群使用不同的网络，保证集群的网络是安全健康的

#### 网络连通

```shell
[root@server ~]# docker network connect --help

Usage:	docker network connect [OPTIONS] NETWORK CONTAINER

Connect a container to a network

Options:
      --alias strings           为容器添加网络范围的别名
      --driver-opt strings      网络的驱动程序选项
      --ip string               IPv4地址（例如172.30.100.104）
      --ip6 string              IPv6地址（例如2001：db8 :: 33）
      --link list               将链接添加到另一个容器
      --link-local-ip strings   为容器添加本地链接地址
#测试打通基于docker0启动的tomcat到基与自定义mynet的tomcat-net01
#启动一个基于docker0网络的tomcat
[root@server ~]# docker run -d -P --name tomcat  tomcat
f0a43d5b98d7ddac9f9f7193dc8cd88c78067a517ccb7b6620568532fe42d8ca
#把基于docker0启动的tomcat添加到mynet网络里
[root@server ~]# docker network connect mynet tomcat
#查看mynet的详情，发现tomcat添加到了Containers里
#也就是一个容器两个ip
[root@server ~]# docker network inspect mynet
12345678910111213141516171819202122
```

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020061511132737.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpeWFuXzA4MTY=,size_16,color_FFFFFF,t_70#pic_center)

**结论**：如果要跨网络操作别人的容器，就需要使用docker network connect 连通

### 实战：部署redis集群

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200615111352520.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpeWFuXzA4MTY=,size_16,color_FFFFFF,t_70#pic_center)

```shell
[root@server ~]# docker network create --subnet 172.38.0.0/16 redis
ac1d5994da35c859ba5b73be9119f86831c1bc66d58f575d64d351ba3f854b28
[root@server ~]# docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
d2a0a2fb88a5        bridge              bridge              local
bf5d1c08782b        host                host                local
1c43dc4178dd        mynet               bridge              local
bbfe8c38fb6a        none                null                local
ac1d5994da35        redis               bridge              local
[root@server ~]# docker network inspect redis 
[
    {
        "Name": "redis",
        "Id": "ac1d5994da35c859ba5b73be9119f86831c1bc66d58f575d64d351ba3f854b28",
        "Created": "2020-05-27T17:38:49.459754017+08:00",
        "Scope": "local",
        "Driver": "bridge",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": {},
            "Config": [
                {
                    "Subnet": "172.38.0.0/16"
                }
            ]
        },
        "Internal": false,
        "Attachable": false,
        "Ingress": false,
        "ConfigFrom": {
            "Network": ""
        },
        "ConfigOnly": false,
        "Containers": {},
        "Options": {},
        "Labels": {}
    }
]

#通过脚本创建6个redis
for port in $(seq 1 6); \
do \
mkdir -p /mydata/redis/node-${port}/conf
touch /mydata/redis/node-${port}/conf/redis.conf
cat << EOF >/mydata/redis/node-${port}/conf/redis.conf
port 6379
bind 0.0.0.0
cluster-enabled yes
cluster-config-file node.conf
cluster-node-timeout 5000
cluster-announce-ip 172.38.0.1${port}
cluster-announce-port 6379
cluster-announce-bus-port 16379
appendonly yes
EOF
done

#启动redis1
docker run -d -p 6371:6379 -p 16371:16379 --name redis1 -v /mydata/redis/node-1/data:/data -v /mydata/redis/node-1/conf/redis.conf:/etc/redis/redis.conf --net redis --ip 172.38.0.11 redis redis-server /etc/redis/redis.conf
#启动redis2
docker run -d -p 6372:6379 -p 16372:16379 --name redis2 -v /mydata/redis/node-2/data:/data -v /mydata/redis/node-2/conf/redis.conf:/etc/redis/redis.conf --net redis --ip 172.38.0.12 redis redis-server /etc/redis/redis.conf
#启动redis3
docker run -d -p 6373:6379 -p 16373:16379 --name redis3 -v /mydata/redis/node-3/data:/data -v /mydata/redis/node-3/conf/redis.conf:/etc/redis/redis.conf --net redis --ip 172.38.0.13 redis redis-server /etc/redis/redis.conf
#启动redis4
docker run -d -p 6374:6379 -p 16374:16379 --name redis4 -v /mydata/redis/node-4/data:/data -v /mydata/redis/node-4/conf/redis.conf:/etc/redis/redis.conf --net redis --ip 172.38.0.14 redis redis-server /etc/redis/redis.conf
#启动redis5
docker run -d -p 6375:6379 -p 16375:16379 --name redis5 -v /mydata/redis/node-5/data:/data -v /mydata/redis/node-5/conf/redis.conf:/etc/redis/redis.conf --net redis --ip 172.38.0.15 redis redis-server /etc/redis/redis.conf
#启动redis6
docker run -d -p 6376:6379 -p 16376:16379 --name redis6 -v /mydata/redis/node-6/data:/data -v /mydata/redis/node-6/conf/redis.conf:/etc/redis/redis.conf --net redis --ip 172.38.0.16 redis redis-server /etc/redis/redis.conf
#进如其中一个redis
[root@server conf]# docker exec -it redis1 /bin/sh
# ls
appendonly.aof	node.conf
#创建集群
# redis-cli --cluster create 172.38.0.11:6379 172.38.0.12:6379 172.38.0.13:6379 172.38.0.14:6379 172.38.0.15:6379 172.38.0.16:6379 --cluster-replicas 1
>>> Performing hash slots allocation on 6 nodes...
Master[0] -> Slots 0 - 5460
Master[1] -> Slots 5461 - 10922
Master[2] -> Slots 10923 - 16383
Adding replica 172.38.0.15:6379 to 172.38.0.11:6379
Adding replica 172.38.0.16:6379 to 172.38.0.12:6379
Adding replica 172.38.0.14:6379 to 172.38.0.13:6379
M: d03cc4ab58f78a8a3bd928ad65f86d424e880a36 172.38.0.11:6379
   slots:[0-5460] (5461 slots) master
M: e2d5435b700ed312d0ae7d9d9d022dcc208c3009 172.38.0.12:6379
   slots:[5461-10922] (5462 slots) master
M: 8214046144f54d258bb52843c6040c6ab87a67ba 172.38.0.13:6379
   slots:[10923-16383] (5461 slots) master
S: 16cb98cbf1d26614f9dcf4fa823026e22cbd19a9 172.38.0.14:6379
   replicates 8214046144f54d258bb52843c6040c6ab87a67ba
S: 80c6e4edf264244190748afc07f7b4084d299c5a 172.38.0.15:6379
   replicates d03cc4ab58f78a8a3bd928ad65f86d424e880a36
S: 0699cb49a006d01f85d3877a2a3f7df9e9b3fd55 172.38.0.16:6379
   replicates e2d5435b700ed312d0ae7d9d9d022dcc208c3009
Can I set the above configuration? (type 'yes' to accept): yes
>>> Nodes configuration updated
>>> Assign a different config epoch to each node
>>> Sending CLUSTER MEET messages to join the cluster
Waiting for the cluster to join
..
>>> Performing Cluster Check (using node 172.38.0.11:6379)
M: d03cc4ab58f78a8a3bd928ad65f86d424e880a36 172.38.0.11:6379
   slots:[0-5460] (5461 slots) master
   1 additional replica(s)
S: 80c6e4edf264244190748afc07f7b4084d299c5a 172.38.0.15:6379
   slots: (0 slots) slave
   replicates d03cc4ab58f78a8a3bd928ad65f86d424e880a36
M: 8214046144f54d258bb52843c6040c6ab87a67ba 172.38.0.13:6379
   slots:[10923-16383] (5461 slots) master
   1 additional replica(s)
M: e2d5435b700ed312d0ae7d9d9d022dcc208c3009 172.38.0.12:6379
   slots:[5461-10922] (5462 slots) master
   1 additional replica(s)
S: 16cb98cbf1d26614f9dcf4fa823026e22cbd19a9 172.38.0.14:6379
   slots: (0 slots) slave
   replicates 8214046144f54d258bb52843c6040c6ab87a67ba
S: 0699cb49a006d01f85d3877a2a3f7df9e9b3fd55 172.38.0.16:6379
   slots: (0 slots) slave
   replicates e2d5435b700ed312d0ae7d9d9d022dcc208c3009
[OK] All nodes agree about slots configuration.
>>> Check for open slots...
>>> Check slots coverage...
[OK] All 16384 slots covered.
# redis-cli -c
=====================================
127.0.0.1:6379> cluster info
cluster_state:ok
cluster_slots_assigned:16384
cluster_slots_ok:16384
cluster_slots_pfail:0
cluster_slots_fail:0
cluster_known_nodes:6
cluster_size:3
cluster_current_epoch:6
cluster_my_epoch:1
cluster_stats_messages_ping_sent:417
cluster_stats_messages_pong_sent:403
cluster_stats_messages_sent:820
cluster_stats_messages_ping_received:398
cluster_stats_messages_pong_received:417
cluster_stats_messages_meet_received:5
cluster_stats_messages_received:820
===========================================
127.0.0.1:6379> cluster nodes
80c6e4edf264244190748afc07f7b4084d299c5a 172.38.0.15:6379@16379 slave d03cc4ab58f78a8a3bd928ad65f86d424e880a36 0 1590575568337 5 connected
8214046144f54d258bb52843c6040c6ab87a67ba 172.38.0.13:6379@16379 master - 0 1590575569539 3 connected 10923-16383
e2d5435b700ed312d0ae7d9d9d022dcc208c3009 172.38.0.12:6379@16379 master - 0 1590575568000 2 connected 5461-10922
d03cc4ab58f78a8a3bd928ad65f86d424e880a36 172.38.0.11:6379@16379 myself,master - 0 1590575567000 1 connected 0-5460
16cb98cbf1d26614f9dcf4fa823026e22cbd19a9 172.38.0.14:6379@16379 slave 8214046144f54d258bb52843c6040c6ab87a67ba 0 1590575569340 4 connected
0699cb49a006d01f85d3877a2a3f7df9e9b3fd55 172.38.0.16:6379@16379 slave e2d5435b700ed312d0ae7d9d9d022dcc208c3009 0 1590575569000 6 connected
================重头戏来了===================
#存储一个键值对
127.0.0.1:6379> set a b
#发现是172.38.0.13:6379为我们处理的
-> Redirected to slot [15495] located at 172.38.0.13:6379
OK
#此时模拟主机redis3出现故障（停掉redis3）
172.38.0.14:6379> get a
"b"
172.38.0.14:6379> CLUSTER nodes
80c6e4edf264244190748afc07f7b4084d299c5a 172.38.0.15:6379@16379 slave d03cc4ab58f78a8a3bd928ad65f86d424e880a36 0 1590576302518 1 connected
0699cb49a006d01f85d3877a2a3f7df9e9b3fd55 172.38.0.16:6379@16379 slave e2d5435b700ed312d0ae7d9d9d022dcc208c3009 0 1590576302417 6 connected
8214046144f54d258bb52843c6040c6ab87a67ba 172.38.0.13:6379@16379 slave,fail 16cb98cbf1d26614f9dcf4fa823026e22cbd19a9 1590576289888 1590576287380 8 connected
d03cc4ab58f78a8a3bd928ad65f86d424e880a36 172.38.0.11:6379@16379 master - 0 1590576303420 1 connected 0-5460
e2d5435b700ed312d0ae7d9d9d022dcc208c3009 172.38.0.12:6379@16379 master - 0 1590576303120 2 connected 5461-10922
16cb98cbf1d26614f9dcf4fa823026e22cbd19a9 172.38.0.14:6379@16379 myself,master - 0 1590576301000 8 connected 10923-16383
#此时172.38.0.13已经fail 由他的从机14补上处理
#继续测试：存储c d键值对，处理的存储的是12，此时我去停掉12，然后获取get c
172.38.0.14:6379> set c d
-> Redirected to slot [7365] located at 172.38.0.12:6379
OK
# redis-cli -c
#此时可以看到是 16处理的get c
127.0.0.1:6379> get c
-> Redirected to slot [7365] located at 172.38.0.16:6379
"d"
#查看redis集群状态，已经挂了两个主机
172.38.0.16:6379> CLUSTER NODES
e2d5435b700ed312d0ae7d9d9d022dcc208c3009 172.38.0.12:6379@16379 master,fail - 1590576450306 1590576448000 2 connected
0699cb49a006d01f85d3877a2a3f7df9e9b3fd55 172.38.0.16:6379@16379 myself,master - 0 1590576634000 9 connected 5461-10922
8214046144f54d258bb52843c6040c6ab87a67ba 172.38.0.13:6379@16379 slave,fail 16cb98cbf1d26614f9dcf4fa823026e22cbd19a9 1590576290388 1590576287882 8 connected
16cb98cbf1d26614f9dcf4fa823026e22cbd19a9 172.38.0.14:6379@16379 master - 0 1590576633000 8 connected 10923-16383
80c6e4edf264244190748afc07f7b4084d299c5a 172.38.0.15:6379@16379 slave d03cc4ab58f78a8a3bd928ad65f86d424e880a36 0 1590576634767 5 connected
d03cc4ab58f78a8a3bd928ad65f86d424e880a36 172.38.0.11:6379@16379 master - 0 1590576633765 1 connected 0-5460
172.38.0.16:6379> 
123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186
```

我们使用了docker之后，所有的技术都变得简单~