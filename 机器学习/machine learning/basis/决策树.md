# 决策树

## basis

![image-20200727145701844](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/19/191001-546773.png)

**可以解决的问题：**

![image-20200727150128861](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/19/191006-701229.png)

## 信息熵

代表不确定性，从物理中延伸过来

![image-20200727150309675](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/19/191010-75273.png)

![image-20200727150334260](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/19/191546-131741.png)

1984年，香农提出‘信息熵’。是度量样本集合纯度的最常用的指标。熵在信息论中代表随机变量不确定度的度量。一条信息大小和它的不确定性有直接关系，要搞清楚一件非常不确定的事情，或者是我们一无所知的事情，需要了解大量信息==>信息的度量就等于不确定的多少。

熵越大，数据不确定性越高，熵越低，数据的不确定性越低。假定当前样本集合D中k类样本所占比例为 ![[公式]](https://www.zhihu.com/equation?tex=p_%7Bi%7D) ，则信息熵的定义：

![[公式]](https://www.zhihu.com/equation?tex=%5Cboxed+%7BH+%3D+-+%5Csum_%7Bi%3D1%7D%5Ek+p_%7Bi%7Dlog%28p_%7Bi%7D%29%7D)

* 假如集合 ![[公式]](https://www.zhihu.com/equation?tex=D_%7B1%7D%3D%5C%7B%5Cfrac%7B1%7D%7B3%7D%2C+%5Cfrac%7B1%7D%7B3%7D%2C+%5Cfrac%7B1%7D%7B3%7D%5C%7D) 则![[公式]](https://www.zhihu.com/equation?tex=H_%7B1%7D+%3D+-%5Cfrac%7B1%7D%7B3%7Dlog%28%5Cfrac%7B1%7D%7B3%7D%29+-%5Cfrac%7B1%7D%7B3%7Dlog%28%5Cfrac%7B1%7D%7B3%7D%29+-%5Cfrac%7B1%7D%7B3%7Dlog%28%5Cfrac%7B1%7D%7B3%7D%29+%3D1.0986)

* 假如集合 ![[公式]](https://www.zhihu.com/equation?tex=D_%7B2%7D%3D%5C%7B%5Cfrac%7B1%7D%7B10%7D%2C+%5Cfrac%7B2%7D%7B10%7D%2C+%5Cfrac%7B7%7D%7B10%7D%5C%7D) 则 ![[公式]](https://www.zhihu.com/equation?tex=H_%7B2%7D+%3D+-%5Cfrac%7B1%7D%7B10%7Dlog%28%5Cfrac%7B1%7D%7B10%7D%29+-%5Cfrac%7B2%7D%7B10%7Dlog%28%5Cfrac%7B2%7D%7B10%7D%29+-%5Cfrac%7B7%7D%7B10%7Dlog%28%5Cfrac%7B7%7D%7B10%7D%29+%3D0.8018+)
* 由于 ![[公式]](https://www.zhihu.com/equation?tex=H_%7B1%7D+%3E+H_%7B2%7D),也就是说 ![[公式]](https://www.zhihu.com/equation?tex=D_%7B2%7D) 比 ![[公式]](https://www.zhihu.com/equation?tex=D_%7B1%7D) 更具有确定性。由于 ![[公式]](https://www.zhihu.com/equation?tex=D_%7B2%7D) 中有很多数据都集中在第三类里，所以是更确定的。而 ![[公式]](https://www.zhihu.com/equation?tex=D_%7B1%7D) 中每一类都占有 ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B3%7D) ,因此不确定性更强。

* 假如集合 ![[公式]](https://www.zhihu.com/equation?tex=D_%7B3%7D+%3D+%5C%7B1%2C+0%2C+0+%5C%7D) ,则 ![[公式]](https://www.zhihu.com/equation?tex=H+%3D+-1+%5Ccdot+log%281%29+%3D+0) ,因为确定所有的数据都在第一类。

以二分类的例， ![[公式]](https://www.zhihu.com/equation?tex=H+%3D+-xlog%28x%29+-+%281-x%29log%281-x%29)

![image-20200727150737575](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/19/191036-733371.png)



**基于信息熵划分**![image-20200727150906767](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/19/191353-120421.png)

由此，我们可以解决问题，就是使得划分后的熵降低。

接下来模拟信息熵进行划分：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
x = iris.data[:, 2:]
y = iris.target

def split(x, y, d, value):
    index_a = (x[:, d] <= value)
    index_b = (x[:, d] > value)
    return x[index_a], x[index_b], y[index_a], y[index_b]

from collections import Counter
from math import log 

def entropy(y):
    counter = Counter(y)
    res = 0.0
    for num in counter.values():
        p = num / len(y)
        res += -p * log(p)
        
    return res

def try_split(x, y):
    
    best_entropy = float('inf')
    best_d, best_v = -1, -1
    # 在使用的鸢尾花数据中只用了后两维，x.shape=(150, 2)
    # 首先将数据进行排序
    for d in range(x.shape[1]):
        sorted_index = np.argsort(x[:,d])
        # 经过排序后，遍历每个划分点，比较划分后的信息熵，从而选择信息增益最大的那个节点
        for i in range(1, len(x)):
            if x[sorted_index[i-1], d] != x[sorted_index[i], d]:
                v = (x[sorted_index[i-1], d] + x[sorted_index[i], d]) / 2
                x_l, x_r, y_l, y_r = split(x, y, d, v)
                e = entropy(y_l) + entropy(y_r)
                if e < best_entropy:
                    best_entropy, best_d, best_v = e, d, v
                    
    return best_entropy, best_d, best_v

best_entropy, best_d, best_v = try_split(x, y)
print('best_entropy=', best_entropy)
print('best_d=', best_d)
print('best_v2',best_v)
# best_entropy= 0.6931471805599453
# best_d= 0
# best_v2 2.45
```





## 基尼系数

与信息熵的性质差不多

![image-20200727153345131](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/19/191401-536195.png)





CART（Classification and Regression Tree）决策树使用基尼指数来选择划分属性，数据集D的纯度可以使用基尼值来度量：

![[公式]](https://www.zhihu.com/equation?tex=%5Cboxed%7BGini%28D%29+%3D+1+-+%5Csum_%7Bi%3D1%7D%5Ek+p_%7Bi%7D%5E2%7D)

类比信息熵，假设有集合 ![[公式]](https://www.zhihu.com/equation?tex=D_%7B1%7D%3D%5C%7B%5Cfrac%7B1%7D%7B3%7D%2C+%5Cfrac%7B1%7D%7B3%7D%2C+%5Cfrac%7B1%7D%7B3%7D%5C%7D) 则

![[公式]](https://www.zhihu.com/equation?tex=G_%7B1%7D+%3D+1+-+%28%5Cfrac%7B1%7D%7B3%7D%29%5E2+-+%28%5Cfrac%7B1%7D%7B3%7D%29%5E2+-+%28%5Cfrac%7B1%7D%7B3%7D%29%5E2+%3D+0.6666)

假设集合 ![[公式]](https://www.zhihu.com/equation?tex=D_%7B2%7D%3D%5C%7B%5Cfrac%7B1%7D%7B10%7D%2C+%5Cfrac%7B2%7D%7B10%7D%2C+%5Cfrac%7B7%7D%7B10%7D%5C%7D) 则

![[公式]](https://www.zhihu.com/equation?tex=G_%7B2%7D+%3D+1+-+%28%5Cfrac%7B1%7D%7B10%7D%29%5E2+-+%28%5Cfrac%7B2%7D%7B10%7D%29%5E2+-+%28%5Cfrac%7B7%7D%7B10%7D%29%5E2+%3D+0.46+)

假设集合 ![[公式]](https://www.zhihu.com/equation?tex=D_%7B3%7D+%3D+%5C%7B1%2C+0%2C+0+%5C%7D) 则

![[公式]](https://www.zhihu.com/equation?tex=G_%7B3%7D+%3D+1+-+1%5E2+%3D+0)

因此， ![[公式]](https://www.zhihu.com/equation?tex=Gini%28D%29) 越小，则数据集D的纯度越高。

同样，类比信息熵以二分类问题为例， ![[公式]](https://www.zhihu.com/equation?tex=G%3D1-x%5E2-%281-x%29%5E2%3D1-x%5E2-1%2B2x-x%5E2%3D-2x%5E2+%2B+2x)

```python
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(0.01, 0.99, 200)
y = -2 * x**2 + 2*x 

plt.plot(x, y)
plt.show()
```

![image-20200727153449206](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/19/191054-174448.png)



### 使用基尼系数绘制决策边界

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

iris = datasets.load_iris()
x = iris.data[:, 2:]
y = iris.target

from sklearn.tree import DecisionTreeClassifier

dt_clf = DecisionTreeClassifier(max_depth=2, criterion='gini')
dt_clf.fit(x, y)

def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(np.linspace(axis[0], axis[1], int((axis[1] - axis[0])*100)).reshape(1, -1),
                         np.linspace(axis[2], axis[3], int((axis[3] - axis[2])*100)).reshape(1, -1),)
    x_new = np.c_[x0.ravel(), x1.ravel()]
    y_predict = model.predict(x_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A', '#FFF59D', '#90CAF9'])

    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)

plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])
plt.scatter(x[y==0, 0], x[y==0, 1])
plt.scatter(x[y==1, 0], x[y==1, 1])
plt.scatter(x[y==2, 0], x[y==2, 1])
plt.show()
```

![image-20200727153822682](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/19/191409-303694.png)

![image-20200727153753090](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/112256-486571.png)

## CART和决策树的超参数

当决策树建立完成后，平均来说模型预测时候的复杂度： ![[公式]](https://www.zhihu.com/equation?tex=O%28logm%29) 其中m表示数据集中的样本个数，但是模型训练的复杂度： ![[公式]](https://www.zhihu.com/equation?tex=O%28n%2Am%2Alogm%29) 。决策树的缺点就是非常容易产生过拟合，其实对于非参数学习来说，比如KNN都很容易产生过拟合。

而决策树中解决过拟合问题的主要手段就是剪枝（pruning）。剪枝的目的是为了降低模型复杂度，解决过拟合。

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

x, y = datasets.make_moons(noise=0.25, random_state=666)
# plt.scatter(x[y==0, 0], x[y==0, 1])
# plt.scatter(x[y==1, 0], x[y==1, 1])
# plt.show()

from sklearn.tree import DecisionTreeClassifier

# 在不传入任何参数时候，默认就是使用gini指数，树的结构分不到不能再分为止。很容易过拟合
dt_clf = DecisionTreeClassifier()
dt_clf.fit(x, y)

def plot_decision_boundary(model, axis):
    x0, x1 = np.meshgrid(np.linspace(axis[0], axis[1], int((axis[1] - axis[0])*100)).reshape(1, -1),
                         np.linspace(axis[2], axis[3], int((axis[3] - axis[2])*100)).reshape(1, -1),)
    x_new = np.c_[x0.ravel(), x1.ravel()]
    y_predict = model.predict(x_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A', '#FFF59D', '#90CAF9'])
    
    plt.contourf(x0, x1, zz, linewidth=5, cmap=custom_cmap)

plot_decision_boundary(dt_clf, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(x[y==0, 0], x[y==0, 1])
plt.scatter(x[y==1, 0], x[y==1, 1])
plt.show()
```

![image-20200727154256284](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/19/191416-770211.png)

第一个超参数：树的深度

```python
dt_clf2 = DecisionTreeClassifier(max_depth=2)
dt_clf2.fit(x, y)

plot_decision_boundary(dt_clf2, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(x[y==0, 0], x[y==0, 1])
plt.scatter(x[y==1, 0], x[y==1, 1])
plt.show()
```

![img](https://pic4.zhimg.com/80/v2-bf8a0323700ea31a17ce9a63d07374f4_720w.jpg)

第二个超参数：最小样本数量

```python
dt_clf3 = DecisionTreeClassifier(min_samples_split=10)
dt_clf3.fit(x, y)

plot_decision_boundary(dt_clf3, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(x[y==0, 0], x[y==0, 1])
plt.scatter(x[y==1, 0], x[y==1, 1])
plt.show()
```

![img](https://pic3.zhimg.com/80/v2-e03bb4faf35ab5426d58ba74b74f0179_720w.jpg)

第三个超参数：叶子节点的最小样本数量

```python
dt_clf3 = DecisionTreeClassifier(min_samples_leaf=6)
dt_clf3.fit(x, y)

plot_decision_boundary(dt_clf3, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(x[y==0, 0], x[y==0, 1])
plt.scatter(x[y==1, 0], x[y==1, 1])
plt.show()
```

![img](https://pic3.zhimg.com/80/v2-47f573bc7d95242f8cfcba550b5f962f_720w.jpg)

第四个超参数：叶子节点的数量

```python
dt_clf4 = DecisionTreeClassifier(max_leaf_nodes=4)
dt_clf4.fit(x, y)

plot_decision_boundary(dt_clf4, axis=[-1.5, 2.5, -1.0, 1.5])
plt.scatter(x[y==0, 0], x[y==0, 1])
plt.scatter(x[y==1, 0], x[y==1, 1])
plt.show()
```

![img](https://pic4.zhimg.com/80/v2-b79d7b6d130568cd819556481a4e602b_720w.jpg)

## 决策树解决回归问题

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

boston = datasets.load_boston()
x = boston.data
y = boston.target

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=666)

from sklearn.tree import DecisionTreeRegressor

dt_reg = DecisionTreeRegressor()
dt_reg.fit(x_train, y_train)
dt_reg.score(x_test, y_test)
# 0.6074066452266129
dt_reg.score(x_train, y_train)
# 1.0
```

## 决策树的局限性

+ 决策边界横平竖直

+ 对某一个数据非常敏感

