# 逻辑回归

## basis

![image-20200722201903514](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/114608-28855.png)

![image-20200722202027573](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/114610-301945.png)

 

![image-20200722202503888](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/114616-945625.png)

**逻辑回归**

![image-20200722202547074](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/114620-570003.png)

## 逻辑回归的损失函数

![image-20200724202130530](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/114629-132530.png)

**妙啊**

![image-20200724202211897](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/114631-901945.png)

**整合**

![image-20200724202339925](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/114636-795638.png)

![image-20200724202419558](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/114637-774016.png)

## 推导损失函数的梯度

pass



## 用梯度下降法求theta

pass





## 决策边界

![[公式]](https://www.zhihu.com/equation?tex=%5Chat+p+%3D+%5Csigma+%28%5Ctheta%5ET+%5Ccdot+X%29+%3D+%5Cfrac%7B1%7D+%7B1+%2B+e%5E%7B-%5Ctheta%5ET+%5Ccdot+X%7D%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5Chat+y+%3D+%5Cbegin+%7Bcases%7D+1%2C+%5Cquad+%5Chat+p+%5Cgeq+0.5+%5C%5C+0%2C+%5Cquad+%5Chat+p+%5Cleq+0.5+%5C%5C+%5Cend+%7Bcases%7D+) ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+%5E+T+%5Ccdot+X)

接下来再看一下Sigmoid这个函数: ![[公式]](https://www.zhihu.com/equation?tex=%5Csigma%28t%29+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-t%7D%7D)

当t>0时，p>0.5，当t<0时，p<0.5

![[公式]](https://www.zhihu.com/equation?tex=%5Chat+p+%3D+%5Csigma+%28%5Ctheta%5ET+%5Ccdot+X%29+%3D+%5Cfrac%7B1%7D+%7B1+%2B+e%5E%7B-%5Ctheta%5ET+%5Ccdot+X%7D%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5Chat+y+%3D+%5Cbegin+%7Bcases%7D+1%2C+%5Cquad+%5Chat+p+%5Cgeq+0.5+%5Cqquad+%5Ctheta%5ET+%5Ccdot+X+%5Cgeq+0++%5C%5C+0%2C+%5Cquad+%5Chat+p+%5Cleq+0.5++%5Cqquad+%5Ctheta%5ET+%5Ccdot+X+%3C+0+%5C%5C+%5Cend+%7Bcases%7D)

由此，得出决策边界 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta%5ET+%5Ccdot+X) 。

假设X有两个特征： ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_%7B0%7D+%2B+%5Ctheta_%7B1%7Dx_%7B1%7D+%2B+%5Ctheta_%7B2%7Dx_%7B2%7D+%3D+0)， ![[公式]](https://www.zhihu.com/equation?tex=x_%7B2%7D+%3D+%5Cfrac%7B-%5Ctheta_%7B0%7D+-+%5Ctheta_%7B1%7Dx_%7B1%7D%7D%7B%5Ctheta_%7B2%7D%7D)

```python
def x2(x1):     
    return (-log_reg.coef_[0] * x1 - log_reg.interception_) / log_reg.coef_[1]

x1_plot = np.linspace(4, 8, 1000)
x2_plot= x2(x1_plot)
plt.scatter(x[y==0, 0], x[y==0, 1], color='red')
plt.scatter(x[y==1, 0], x[y==1, 1], color='blue')
plt.plot(x1_plot, x2_plot)
plt.show()
```

![img](https://picb.zhimg.com/80/v2-612e516a6bb6ceb28c8eef588b605103_720w.jpg)

## 逻辑回归中使用多项式特征

有过拟合的问题

### 使用正则化进行解决

pass 与前面的模型泛化差不多





## OvR OvO

两种不同方式实现逻辑回归的分类



