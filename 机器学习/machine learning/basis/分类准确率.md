# 分类算法的评价

## 混淆矩阵

![image-20200726000424164](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111834-660397.png)



![image-20200726000510881](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111837-581685.png)

### 精准率

![image-20200726000712161](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111839-782365.png)

将1作为最关注的对象

### 召回率

![image-20200726000803366](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111917-588694.png)

**理解**：

![image-20200726000922553](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111903-434473.png)

**精准率和召回率的意义**：

![image-20200726001203272](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111922-399031.png)



## 精准率和召回率的平衡

 ![image-20200726143143802](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111842-145598.png)

![image-20200726143234275](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111845-833084.png)

可以看出精准率和召回率之间是互相矛盾的



### PR曲线

**thresholds的移动**

**1、编程实现绘制PR曲线**

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

digits = datasets.load_digits()
x = digits.data
y = digits.target.copy()
# 生成不平衡的数据
y[digits.target==9] = 1
y[digits.target!=9] = 0
x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=666)
log_reg = LogisticRegression()
log_reg.fit(x_train, y_train)

decision_scores = log_reg.decision_function(x_test)
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

precisions = []
recalls = []
thresholds = np.arange(np.min(decision_scores), np.max(decision_scores))

for threshold in thresholds:
    y_predict = np.array(decision_scores >= threshold, dtype='int')
    precisions.append(precision_score(y_test, y_predict))
    recalls.append(recall_score(y_test, y_predict)) 
    
plt.plot(thresholds, precisions, label='precision')
plt.plot(thresholds, recalls, label='recall')
plt.legend()
plt.show()
```

![image-20200726153039524](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111849-334525.png)

```python
plt.plot(precisions, recalls)
plt.show()
```

![image-20200726153136444](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111857-883765.png)

**2、sklearn中实现P-R曲线**

```python
from sklearn.metrics import precision_recall_curve

precisions, recalls, thresholds = precision_recall_curve(y_test, decision_scores)
precisions.shape
# (145,)
recalls.shape
# (145,)
thresholds.shape
# (144,)
```

*通过上面程序输出结果可以发现返回的精准率和召回率与阈值的长度不一致，这是因为在sklearn中会自动取合适的阈值范围内计算准确率和召回率，而且**默认的最大值为1和最小值为0，没有对应的threshold**，因此这就是为什么thresholds比precisions和recalls长度少1，因此在绘图的时候需要注意。*

```python
plt.plot(thresholds, precisions[:-1])
plt.plot(thresholds, recalls[:-1])
plt.show()
```

![image-20200726153325340](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111933-351646.png)

```python
plt.plot(precisions, recalls)
plt.show()
```

![image-20200726153352415](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111937-362810.png)

*通过精准率和召回率曲线，可以确定合理的阈值去平衡精准率与召回率他们之间的变化关系。而PR曲线中急剧下降的点大概就是最佳平衡点。最后假如使用两种不同的算法绘制出的PR曲线如下图所示，那么哪种算法更优呢？*



**整体的曲线**

* 

![image-20200726153418540](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111939-604103.png)

*很显然，外面那根曲线上的每一个点都比里面那根曲线的precisions和recalls大，所以整体来说如果PR曲线**更靠外，也就更好**，因此也可以作为选择算法选择超参数的一种指标。其实就是PR曲线下的面积，来衡量模型的优劣，但是一般情况下都会使用另外一种曲线下的**面积**。由曲线的下的面积，引出下一个知识点ROC曲线。*

### ROC曲线

*ROC曲线(Receiver Operation Characteristic Curve)，描述TPR和FPR之间的关系。**接受者操作特性曲线**是指在特定刺激条件下，以被试在不同判断标准下所得的虚报概率P（y/N）为横坐标，以击中概率P（y/SN）为纵坐标，画得的各点的连线。*



![](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111944-63191.png)

![image-20200726153753226](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111946-768443.png)

1、 ![[公式]](https://www.zhihu.com/equation?tex=TPR%3D%5Cfrac%7BTP%7D%7BTP%2BFN%7D+%3D+Recall) ，其实就是预测结果为1且正确的数量占真实为1的数量的结果。

2、 ![[公式]](https://www.zhihu.com/equation?tex=FPR%3D%5Cfrac%7BFP%7D%7BTN%2BFP%7D) ，就是预测结果为1但错误的数量占真实值为0的数量的结果。

3、TPR和FPR的关系

![img](https://pic4.zhimg.com/80/v2-3ad3a5a9d5657a6e73850313f3432f6c_720w.jpg)

 **编程实现：**

```python
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

def TN(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 0) & (y_predict == 0))

def FP(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 0) & (y_predict == 1))

def FN(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 1) & (y_predict == 0))

def TP(y_true, y_predict):
    assert len(y_true) == len(y_predict)
    return np.sum((y_true == 1) & (y_predict == 1))

def TPR(y_true, y_predict):
    tp = TP(y_true, y_predict)
    fn = FN(y_true, y_predict)
    try:
        return tp / (tp + fn)
    except:
        return 0.0
    
def FPR(y_true, y_predict):
    fp = FP(y_true, y_predict)
    tn = TN(y_true, y_predict)
    try:
        return fp / (fp + tn)
    except:
        return 0.0
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split

digits = datasets.load_digits()
x = digits.data
y = digits.target.copy()
# 生成不平衡的数据
y[digits.target==9] = 1
y[digits.target!=9] = 0

x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=666)

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(x_train, y_train)
log_reg.score(x_test, y_test)
# 0.9755555555555555
log_reg_predict = log_reg.predict(x_test)

decision_scores = log_reg.decision_function(x_test)

fprs = []
tprs = []
thresholds = np.arange(np.min(decision_scores), np.max(decision_scores))

for threshold in thresholds:
    y_predict = np.array(decision_scores >= threshold, dtype='int')
    fprs.append(FPR(y_test, y_predict))
    tprs.append(TPR(y_test, y_predict))
    
plt.plot(fprs, tprs)
plt.show()
```

![image-20200726154235517](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111952-999857.png)

**sklearn中ROC曲线的实现：**

```python
from sklearn.metrics import roc_curve

fprs, tprs, thresholds = roc_curve(y_test, decision_scores)
plt.plot(fprs, tprs)
plt.show()
```

![image-20200726154314361](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111954-718639.png)

*曲线下的面积越大，说明模型的分类效果越好，这是因为在ROC曲线刚开始，**fpr较低（预测为1的错误越低）的时候，tpr越大（预测为1正确的越多），曲线下的面积越大，分类算法的模型也就更好**。 由输出结果可以发现ROC的AUC值对不均衡样本不是那么敏感，因此对于极度有偏的数据集查看模型的精准率和召回率曲线还是很有必要的，ROC的AUC的主要应用是比较模型或者算法的优劣。*

![image-20200726154559606](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/111957-561503.png)

## **多分类问题中的混淆矩阵**

```python
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split

digits = datasets.load_digits()
x = digits.data
y = digits.target

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.8, random_state=666)

from sklearn.linear_model import LogisticRegression

log_reg = LogisticRegression()
log_reg.fit(x_train, y_train)
log_reg.score(x_test, y_test)

y_predict = log_reg.predict(x_test)

from sklearn.metrics import precision_score
precision_score(y_test, y_predict, average='micro')
# 可以尝试一下precision_score(y_test, y_predict)，默认情况下是不支持多分类准确率预测的，但是可以通过传入超参数解决
# 0.93115438108484
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_predict)
```

输出结果：

```python
array([[147,   0,   1,   0,   0,   1,   0,   0,   0,   0],
       [  0, 123,   1,   2,   0,   0,   0,   3,   4,  10],
       [  0,   0, 134,   1,   0,   0,   0,   0,   1,   0],
       [  0,   0,   0, 138,   0,   5,   0,   1,   5,   0],
       [  2,   5,   0,   0, 139,   0,   0,   3,   0,   1],
       [  1,   3,   1,   0,   0, 146,   0,   0,   1,   0],
       [  0,   2,   0,   0,   0,   1, 131,   0,   2,   0],
       [  0,   0,   0,   1,   0,   0,   0, 132,   1,   2],
       [  1,   9,   2,   3,   2,   4,   0,   0, 115,   4],
       [  0,   1,   0,   5,   0,   3,   0,   2,   2, 134]], dtype=int64)
```

这样看上去并不直观，绘制一下混淆矩阵。

```python
import matplotlib.pyplot as plt
cfm = confusion_matrix(y_test, y_predict)
plt.matshow(cfm, cmap=plt.cm.gray)
plt.show()
```

![image-20200726155045538](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/112000-756719.png)

图中白色方框越亮说明预测正确率越高，但是如果只是显示正确率对于混淆矩阵并能说明什么，是没有意义的，其实我们是想看看预测错误部分。

```python
# 计算每一行有多少个样本
row_sums = np.sum(cfm, axis=1)
err_matrix = cfm / row_sums
# 不关注预测正确的那部分，将对角线填成0
np.fill_diagonal(err_matrix, 0)
err_matrix
```

输出结果：

```python
array([[0.        , 0.        , 0.00735294, 0.        , 0.        ,
        0.00657895, 0.        , 0.        , 0.        , 0.        ],
       [0.        , 0.        , 0.00735294, 0.01342282, 0.        ,
        0.        , 0.        , 0.02205882, 0.02857143, 0.06802721],
       [0.        , 0.        , 0.        , 0.00671141, 0.        ,
        0.        , 0.        , 0.        , 0.00714286, 0.        ],
       [0.        , 0.        , 0.        , 0.        , 0.        ,
        0.03289474, 0.        , 0.00735294, 0.03571429, 0.        ],
       [0.01342282, 0.03496503, 0.        , 0.        , 0.        ,
        0.        , 0.        , 0.02205882, 0.        , 0.00680272],
       [0.00671141, 0.02097902, 0.00735294, 0.        , 0.        ,
        0.        , 0.        , 0.        , 0.00714286, 0.        ],
       [0.        , 0.01398601, 0.        , 0.        , 0.        ,
        0.00657895, 0.        , 0.        , 0.01428571, 0.        ],
       [0.        , 0.        , 0.        , 0.00671141, 0.        ,
        0.        , 0.        , 0.        , 0.00714286, 0.01360544],
       [0.00671141, 0.06293706, 0.01470588, 0.02013423, 0.01333333,
        0.02631579, 0.        , 0.        , 0.        , 0.02721088],
       [0.        , 0.00699301, 0.        , 0.03355705, 0.        ,
        0.01973684, 0.        , 0.01470588, 0.01428571, 0.        ]])
```

这个输出就是预测错误部分，整体看上去还是挺费劲的，然后绘制一下这个矩阵。

```python
plt.matshow(err_matrix, cmap=plt.cm.gray)
plt.show()
```

![image-20200726155336228](https://raw.githubusercontent.com/yanzhenxing123/blogImg/master/typora202008/22/112003-127613.png)

**越亮的部分就是预测错误越多的地方**

*可以看出很容易混淆1和9，也很容易混淆1和8，可以进行微调进行改进。*

