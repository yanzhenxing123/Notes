# 报告

学号姓名学号姓名



## 安装flume

https://blog.csdn.net/vbirdbest/article/details/104479258

### 配置

/usr/local/Cellar/flume/1.9.0_1/libexec

建立stream.conf

命令: `bin/flume-ng agent --conf conf --conf-file stream.conf --name a1 -Dflume.root.logger=INFO,console`

```yaml
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1
# Describe/configure the source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /Users/yanzx/Files/algorithms/cloud_computing/data/vegetable.log
a1.sources.r1.shell = /bin/sh -c
# Describe the sink
a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink
a1.sinks.k1.kafka.bootstrap.servers = localhost:9092
a1.sinks.k1.topic = streamtopic
a1.sinks.k1.batchSize = 25
# a1.sinks.k1.

a1.channels.c1.type = memory

a1.sources.r1.channels = c1      
a1.sinks.k1.channel = c1   
```



### maven依赖

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>top.yanzx</groupId>
    <artifactId>cloud_computing</artifactId>
    <version>1.0-SNAPSHOT</version>

    <properties>
        <maven.compiler.source>8</maven.compiler.source>
        <maven.compiler.target>8</maven.compiler.target>
    </properties>

    <dependencies>
        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.12</artifactId>
            <version>3.1.2</version>
        </dependency><!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java -->

        <dependency>
            <groupId>mysql</groupId>
            <artifactId>mysql-connector-java</artifactId>
            <version>8.0.26</version>
        </dependency>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.12</artifactId>
            <version>3.1.2</version>
        </dependency>

        <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-10 -->
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming-kafka-0-10_2.12</artifactId>
            <version>3.1.2</version>
        </dependency>

    </dependencies>


    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.1</version>
            </plugin>
            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <configuration>
                    <archive>
                        <manifest>
                            <mainClass>com.test.app</mainClass>
                        </manifest>
                    </archive>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <!--下面是为了使用 mvn package命令，如果不加则使用mvn assembly-->
                <executions>
                    <execution>
                        <id>make-assemble</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
</project>
```





spark-submit --class top.yanzx.SparkStreamingFromkafkaDirect cloud_computing-1.0-SNAPSHOT-jar-with-dependencies.jar /

![image-20210921220507582](https://i.loli.net/2021/09/21/FMsqJD8jcbAlwfQ.png)

## kafka

### 启动zookeeper

后台启动zookeeper

![image-20210921211918762](https://i.loli.net/2021/09/21/opAamsiB8w9Rq2L.png)

### 启动kafka

nohup kafka-server-start /usr/local/etc/kafka/server.properties &

![image-20210921212111763](https://i.loli.net/2021/09/21/AN7IWVgS1aoCjwG.png)

查看启动的状态

![image-20210921212138192](https://i.loli.net/2021/09/21/EamBznxNoARyc7G.png)

### 创建topic（streamtopic）

kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic streamtopic

![image-20210921212308752](https://i.loli.net/2021/09/21/CJaNMX2kL3zvqbd.png)

### 创建消费者 streamtopic

kafka-console-consumer --bootstrap-server localhost:9092 -topic streamtopic

创建消费者订阅主题（streamtopic）

![image-20210921212420521](https://i.loli.net/2021/09/21/keZGz7tTvdusVIQ.png)

### 创建生产者

kafka-console-producer --topic topic_a --broker-list localhost:9092

### 查看topics

kafka-topics --list --zookeeper localhost:2181

![image-20210921212456735](https://i.loli.net/2021/09/21/3GeYW9BM6Z8Lltr.png)

## mysql

### 创建数据库

```sql
 CREATE DATABASE IF NOT EXISTS cloud_computing DEFAULT CHARSET utf8;
```



### 创建表

```sql
create table `data_vegetable` (
    `id` int (11) NOT NULL PRIMARY KEY AUTO_INCREMENT,
    `date` varchar(255) NULL,
  	`variety`  varchar(255) NULL,
  	`price` float(10) NULL
); 
```



## java代码

消费streamtopic

```java
package top.yanzx;

import java.sql.Connection;
import java.sql.PreparedStatement;
import java.sql.SQLException;
import java.util.*;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.common.TopicPartition;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.api.java.function.VoidFunction;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka010.ConsumerStrategies;
import org.apache.spark.streaming.kafka010.KafkaUtils;
import org.apache.spark.streaming.kafka010.LocationStrategies;

import scala.Tuple2;


public class SparkStreamingFromkafkaDirect {
    /**
     * 使用Direct 的方式拉取kafka 数据 从指定的offset开始消费  并且打印出每一条数据的分区id和偏移量以及key value值
     */
    public static void main(String[] args) throws Exception {
        // TODO Auto-generated method stub
        //声明conf
        SparkConf sparkConf = new SparkConf().setMaster("local[*]").setAppName("SparkStreamingFromkafka");
        // 窗口间隔
        JavaStreamingContext streamingContext = new JavaStreamingContext(sparkConf, Durations.seconds(5));
        // kafka 参数
        Map<String, Object> kafkaParams = new HashMap<>();
        kafkaParams.put("bootstrap.servers", "127.0.0.1:9092");
        kafkaParams.put("key.deserializer", StringDeserializer.class);
        kafkaParams.put("value.deserializer", StringDeserializer.class);
        kafkaParams.put("group.id", "sparkStreaming" + Math.random());
        // 配置主题
        Collection<String> topics = Collections.singletonList("streamtopic");
        // 配置对应的主题分区的offset，从指定offset消费
        Map<TopicPartition, Long> topicsAndOffset = new HashMap<>();
        topicsAndOffset.put(new TopicPartition("streamtopic", 0), 0L);
        //声明 streaming-kafka-Direct 方式拉取数据  分区一一对应
        JavaInputDStream<ConsumerRecord<String, String>> javaInputDStream = KafkaUtils.createDirectStream(
                streamingContext,
                LocationStrategies.PreferConsistent(), //分区策略
                ConsumerStrategies.Subscribe(topics, kafkaParams, topicsAndOffset) //消费者策略   方式二 从每个分区的 指定的offset 开始消费
        );

        // 将kafka中的数据拉去出来
        JavaPairDStream<String, String> javaPairDStream = javaInputDStream.mapToPair(new PairFunction<ConsumerRecord<String, String>, String, String>() {
            private static final long serialVersionUID = 1L;
            @Override
            //获取数据中的 key和value  (ConsumerRecord 保存了完整的kafka中信息  包括分区 偏移量 等)
            public Tuple2<String, String> call(ConsumerRecord<String, String> consumerRecord) throws Exception {
//                System.out.println("--------------- ConsumerRecord ------------------------ ");
//                System.out.println("partition:   "+consumerRecord.partition());
//                System.out.println("offset:   "+consumerRecord.offset());
//                System.out.println("toString:   "+consumerRecord.toString());
                // 打印日志
                Thread.sleep(10000);
                return new Tuple2<>(consumerRecord.key(), consumerRecord.value());
            }
        });

        //使用action算子打印出value值（主要是为了触发行动算子）
        javaPairDStream.foreachRDD(new VoidFunction<JavaPairRDD<String, String>>() {
            @Override
            public void call(JavaPairRDD<String, String> javaPairRDD) throws Exception {
                javaPairRDD.foreach(new VoidFunction<Tuple2<String, String>>() {
                    @Override
                    public void call(Tuple2<String, String> tuple2) throws Exception {
                        System.out.println(tuple2._2);
                        insertData(tuple2._2);
                    }
                });
            }
        });
        // 启动streaming 应用
        streamingContext.start();
        // 等待处理结束的信号
        streamingContext.awaitTermination();
    }

    public static boolean insertData(String dataPiece) throws SQLException {
        /*
         插入数据
         */
        String[] dataArray = dataPiece.split(" ");
        String date = dataArray[0];
        String variety = dataArray[1];
        float price = Float.parseFloat(dataArray[5]);
        String insertSql = "insert into data_vegetable(`date`, `variety`, `price`) values (?, ?, ?)";
        Connection conn = MysqlClient.getConn();
        PreparedStatement preparedStatement = conn.prepareStatement(insertSql);
        preparedStatement.setString(1, date);
        preparedStatement.setString(2, variety);
        preparedStatement.setFloat(3, price);
        preparedStatement.execute();
        return true;
    }
}
```

MysqlClient.java

```java
package top.yanzx;

import java.sql.*;

public class MysqlClient {
    public static String url = "jdbc:mysql://127.0.0.1:3306/cloud_computing?characterEncoding=UTF-8";
    public static String user = "root";
    public static String pwd = "5211314yzx";

    public static Connection getConn() {
        try {
            return DriverManager.getConnection(url, user, pwd);
        } catch (SQLException throwables) {
            throwables.printStackTrace();
        }
        return null;
    }

    public static void main(String[] args) throws SQLException {
        String dataPiece = "2020-11-19 大白菜 重庆双福国际农贸城 1.00 1.60 1.30 元/千克(kg)";
        String[] dataArray = dataPiece.split(" ");
        String date = dataArray[0];
        String variety = dataArray[1];
        float price = Float.parseFloat(dataArray[5]);
        String insertSql = "insert into data_vegetable(`date`, `variety`, `price`) values (?, ?, ?)";
        Connection conn = MysqlClient.getConn();
        PreparedStatement preparedStatement = conn.prepareStatement(insertSql);
        preparedStatement.setString(1, date);
        preparedStatement.setString(2, variety);
        preparedStatement.setFloat(3, price);
        preparedStatement.execute();
    }
}
```

## Python

### 爬虫代码

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
import sys
import requests
import urllib.request
import urllib.parse
from lxml import etree


class Response:
    def __init__(self, url, data=None):
        self.url = url
        self.data = data
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.164 Mobile Safari/537.36"}

    def urllib_req(self):
        if self.data:
            data = bytes(urllib.parse.urlencode(self.data), encoding='utf-8')
            req = urllib.request.Request(url=self.url, headers=self.headers, data=data)
            response = urllib.request.urlopen(req)
            return response
        else:
            req = urllib.request.Request(url=self.url, headers=self.headers)
            response = urllib.request.urlopen(req)
            return response

    def requests_req(self):
        if self.data:
            response = requests.post(url=self.url, headers=self.headers, data=self.data)
            response.encoding = 'utf-8'
            return response
        else:
            response = requests.get(url=self.url, headers=self.headers)
            response.encoding = 'utf-8'
            return response

    def analy(self):
        html = etree.HTML(self.requests_req().text)
        return html


f = open("./data/vegetable.log", "w+")
for page in range(100, 1, -1):
    url = 'http://www.vegnet.com.cn/Price/List_ar500000_p' + str(page) + '.html?marketID=0'
    response = Response(url)
    html = response.analy()
    info = html.xpath('//div[@class="pri_k"]/p')
    for i in info:
        date = i.xpath('./span[1]/text()')[0][1:][:-1]
        varity = i.xpath('./span[2]/text()')[0]
        market = i.xpath('./span[3]/a/text()')[0]
        price_min = i.xpath('./span[4]/text()')[0][1:]
        price_max = i.xpath('./span[5]/text()')[0][1:]
        price_average = i.xpath('./span[6]/text()')[0][1:]
        unit = i.xpath('./span[7]/text()')[0]
        query_log = "{date} {varity} {market} {price_min} {price_max} {price_average} {unit} ".format(date=date,
                                                                                                      varity=varity,
                                                                                                      market=market,
                                                                                                      price_min=price_min,
                                                                                                      price_max=price_max,
                                                                                                      price_average=price_average,
                                                                                                      unit=unit)
        f.write(query_log + "\n")


        print("main")
        print("main")


        print("local")
        print("local")
```

![image-20210921212917255](https://i.loli.net/2021/09/21/tgw2Ormac1TPXLY.png)

![image-20210921212937225](https://i.loli.net/2021/09/21/Uj6NPnWXhgo1ac8.png)

### 机器学习

```python
"""
@Author: yanzx
@Date: 2021-09-16 09:59:37
@Desc: 
"""

import pandas as pd
import numpy as np
import pandas_datareader
import itertools
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.pylab import style
from statsmodels.tsa.arima_model import ARIMA
import statsmodels.api as sm


style.use('ggplot')
plt.rcParams['font.sans-serif']=['SimHei']
plt.rcParams['axes.unicode_minus'] = False
stockFile='./data_vegetable.csv'
data=pd.read_csv(stockFile,index_col=0,parse_dates=[0])
data.tail(10)
data=data[['price']].resample('W-MON').mean()
data=data.fillna(data.bfill())
data_train=data['2017':'2021']
data_train.plot(figsize=(15,8))
plt.legend(loc='upper left')
plt.title('vegetable price')
plt.xlabel('日期')
plt.ylabel('蔬菜价格')
sns.despine()
p = d = q = range(0, 2)
pdq = list(itertools.product(p, d, q))
seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]
print('季节性 ARIMA 的参数选择:')
print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))
print('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))
print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))
print('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))

warnings.filterwarnings("ignore")  # 忽视警告提示信息
for param in pdq:
    for param_seasonal in seasonal_pdq:
        mod = sm.tsa.statespace.SARIMAX(data_train,
                                    order=param,
                                    seasonal_order=param_seasonal,
                                    enforce_stationarity=False, 
                                        enforce_invertibility=False)
        results = mod.fit()
        print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))
model = sm.tsa.statespace.SARIMAX(data_train,
        order=(1, 0, 1),
        seasonal_order=(0, 1, 1, 12),
#         enforce_stationarity=False,
        enforce_invertibility=False)
results = model.fit()
print(results.summary())
results.plot_diagnostics(figsize=(15, 12))
plt.show()


pred=results.predict('2020','20210703',
                     dynamic=False,
                     typ='levels')
print(pred)
plt.figure(figsize=(15,4))
plt.xticks(rotation=45)
plt.plot(pred,label='Forecast')
plt.plot(data_train,label='True')
plt.xlabel('日期')
plt.ylabel('蔬菜价格')
plt.legend(loc='upper left')
plt.title('季节性 ARIMA--预测蔬菜价格随日期的变化关系')
plt.show()
```

![image-20210921220612090](https://i.loli.net/2021/09/21/qUnmirDk8cp5ECA.png)

![image-20210921220623610](https://i.loli.net/2021/09/21/BspMPYcrxqm8ROQ.png)

![image-20210921220642434](https://i.loli.net/2021/09/21/79IeomLj5SZR8zU.png)
